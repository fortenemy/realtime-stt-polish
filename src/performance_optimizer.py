"""
Performance Optimizer for Real-time STT
Optymalizator wydajnoÅ›ci dla Real-time STT

Autor: AI Assistant
Data: 2025-01-18
"""

import gc
import threading
import time
import psutil
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """Metryki wydajnoÅ›ci systemu"""
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    gpu_memory_used_mb: Optional[float]
    gpu_memory_total_mb: Optional[float]
    active_threads: int
    queue_sizes: Dict[str, int]
    processing_times: Dict[str, float]
    
    @property
    def memory_usage_ratio(self) -> float:
        """Stosunek uÅ¼ywanej pamiÄ™ci do dostÄ™pnej"""
        return self.memory_used_mb / max(self.memory_available_mb, 1)
    
    @property
    def is_memory_critical(self) -> bool:
        """Czy pamiÄ™Ä‡ jest na krytycznym poziomie"""
        return self.memory_percent > 85.0
    
    @property
    def is_cpu_overloaded(self) -> bool:
        """Czy CPU jest przeciÄ…Å¼ony"""
        return self.cpu_percent > 80.0

class PerformanceOptimizer:
    """
    Optymalizator wydajnoÅ›ci dla Real-time STT
    """
    
    def __init__(
        self,
        enable_monitoring: bool = True,
        optimize_memory: bool = True,
        optimize_threads: bool = True,
        monitoring_interval: float = 1.0
    ):
        """
        Inicjalizacja optimizera
        
        Args:
            enable_monitoring: Czy wÅ‚Ä…czyÄ‡ monitoring wydajnoÅ›ci
            optimize_memory: Czy wÅ‚Ä…czyÄ‡ optymalizacjÄ™ pamiÄ™ci
            optimize_threads: Czy wÅ‚Ä…czyÄ‡ optymalizacjÄ™ wÄ…tkÃ³w
            monitoring_interval: InterwaÅ‚ monitoringu w sekundach
        """
        self.enable_monitoring = enable_monitoring
        self.optimize_memory = optimize_memory
        self.optimize_threads = optimize_threads
        self.monitoring_interval = monitoring_interval
        
        # Stan monitoringu
        self.is_monitoring = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.metrics_history: List[PerformanceMetrics] = []
        self.max_history_size = 60  # 1 minuta przy 1s interwale
        
        # Optymalizacje
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self.memory_threshold = 80.0  # %
        self.cpu_threshold = 75.0     # %
        
        # Callbacks
        self.performance_callbacks = []
        
        logger.info(f"ğŸš€ PerformanceOptimizer zainicjalizowany")
    
    def start_monitoring(self):
        """Rozpocznij monitoring wydajnoÅ›ci"""
        if self.is_monitoring:
            logger.warning("âš ï¸ Monitoring juÅ¼ uruchomiony")
            return
        
        if not self.enable_monitoring:
            logger.info("ğŸ“Š Monitoring wyÅ‚Ä…czony")
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="PerformanceMonitor"
        )
        self.monitoring_thread.start()
        
        logger.info("ğŸ“Š Monitoring wydajnoÅ›ci uruchomiony")
    
    def stop_monitoring(self):
        """Zatrzymaj monitoring wydajnoÅ›ci"""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=2.0)
        
        logger.info("ğŸ“Š Monitoring wydajnoÅ›ci zatrzymany")
    
    def _monitoring_loop(self):
        """GÅ‚Ã³wna pÄ™tla monitoringu"""
        while self.is_monitoring:
            try:
                # Zbierz metryki
                metrics = self.collect_metrics()
                
                # Dodaj do historii
                self.metrics_history.append(metrics)
                if len(self.metrics_history) > self.max_history_size:
                    self.metrics_history.pop(0)
                
                # SprawdÅº czy potrzeba optymalizacji
                self._check_optimization_triggers(metrics)
                
                # WywoÅ‚aj callbacks
                for callback in self.performance_callbacks:
                    try:
                        callback(metrics)
                    except Exception as e:
                        logger.error(f"âŒ Performance callback error: {e}")
                
                # Oczekaj do nastÄ™pnego cyklu
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"âŒ Monitoring loop error: {e}")
                time.sleep(self.monitoring_interval)
    
    def collect_metrics(self) -> PerformanceMetrics:
        """Zbierz aktualne metryki wydajnoÅ›ci"""
        try:
            # CPU i pamiÄ™Ä‡ systemu
            cpu_percent = psutil.cpu_percent(interval=None)
            memory = psutil.virtual_memory()
            
            # GPU metrics (jeÅ›li dostÄ™pne)
            gpu_memory_used = None
            gpu_memory_total = None
            
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory_used = torch.cuda.memory_allocated() / 1024**2  # MB
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
            except ImportError:
                pass
            
            # WÄ…tki
            active_threads = threading.active_count()
            
            # Queue sizes (jeÅ›li dostÄ™pne)
            queue_sizes = self._get_queue_sizes()
            
            # Processing times (placeholder)
            processing_times = self._get_processing_times()
            
            return PerformanceMetrics(
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / 1024**2,
                memory_available_mb=memory.available / 1024**2,
                gpu_memory_used_mb=gpu_memory_used,
                gpu_memory_total_mb=gpu_memory_total,
                active_threads=active_threads,
                queue_sizes=queue_sizes,
                processing_times=processing_times
            )
            
        except Exception as e:
            logger.error(f"âŒ Metrics collection error: {e}")
            # ZwrÃ³Ä‡ domyÅ›lne metryki
            return PerformanceMetrics(
                cpu_percent=0.0,
                memory_percent=0.0,
                memory_used_mb=0.0,
                memory_available_mb=1000.0,
                gpu_memory_used_mb=None,
                gpu_memory_total_mb=None,
                active_threads=1,
                queue_sizes={},
                processing_times={}
            )
    
    def _get_queue_sizes(self) -> Dict[str, int]:
        """Pobierz rozmiary kolejek (implementacja specyficzna dla pipeline)"""
        # TODO: Integracja z RealtimePipeline
        return {}
    
    def _get_processing_times(self) -> Dict[str, float]:
        """Pobierz czasy przetwarzania"""
        # TODO: Integracja z komponentami
        return {}
    
    def _check_optimization_triggers(self, metrics: PerformanceMetrics):
        """SprawdÅº czy potrzeba optymalizacji"""
        try:
            # Memory optimization
            if self.optimize_memory and metrics.is_memory_critical:
                logger.warning(f"âš ï¸ Krytyczny poziom pamiÄ™ci: {metrics.memory_percent:.1f}%")
                self.optimize_memory_usage()
            
            # CPU optimization
            if metrics.is_cpu_overloaded:
                logger.warning(f"âš ï¸ PrzeciÄ…Å¼enie CPU: {metrics.cpu_percent:.1f}%")
                self.optimize_cpu_usage()
            
            # GPU memory optimization
            if (metrics.gpu_memory_used_mb and metrics.gpu_memory_total_mb and
                metrics.gpu_memory_used_mb / metrics.gpu_memory_total_mb > 0.9):
                logger.warning("âš ï¸ Krytyczny poziom pamiÄ™ci GPU")
                self.optimize_gpu_memory()
                
        except Exception as e:
            logger.error(f"âŒ Optimization trigger error: {e}")
    
    def optimize_memory_usage(self):
        """Optymalizuj uÅ¼ycie pamiÄ™ci"""
        try:
            logger.info("ğŸ§¹ Optymalizacja pamiÄ™ci...")
            
            # WymuÅ› garbage collection
            collected = gc.collect()
            logger.info(f"ğŸ—‘ï¸ Garbage collection: {collected} obiektÃ³w")
            
            # GPU memory cleanup (jeÅ›li dostÄ™pne)
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("ğŸ”¥ GPU cache wyczyszczony")
            except ImportError:
                pass
            
            # TODO: Dodatkowe optymalizacje specyficzne dla aplikacji
            
        except Exception as e:
            logger.error(f"âŒ Memory optimization error: {e}")
    
    def optimize_cpu_usage(self):
        """Optymalizuj uÅ¼ycie CPU"""
        try:
            logger.info("âš¡ Optymalizacja CPU...")
            
            # TODO: Implementacja optymalizacji CPU
            # - Redukcja czÄ™stotliwoÅ›ci prÃ³bkowania
            # - ZwiÄ™kszenie chunk size
            # - Ograniczenie liczby wÄ…tkÃ³w
            
        except Exception as e:
            logger.error(f"âŒ CPU optimization error: {e}")
    
    def optimize_gpu_memory(self):
        """Optymalizuj pamiÄ™Ä‡ GPU"""
        try:
            logger.info("ğŸ”¥ Optymalizacja pamiÄ™ci GPU...")
            
            import torch
            if torch.cuda.is_available():
                # WyczyÅ›Ä‡ cache
                torch.cuda.empty_cache()
                
                # TODO: Dodatkowe optymalizacje GPU
                # - Redukcja batch size
                # - UÅ¼ycie half precision
                
        except ImportError:
            pass
        except Exception as e:
            logger.error(f"âŒ GPU optimization error: {e}")
    
    def get_current_metrics(self) -> Optional[PerformanceMetrics]:
        """Pobierz aktualne metryki"""
        if self.metrics_history:
            return self.metrics_history[-1]
        return self.collect_metrics()
    
    def get_metrics_history(self, last_n: Optional[int] = None) -> List[PerformanceMetrics]:
        """Pobierz historiÄ™ metryki"""
        if last_n is None:
            return self.metrics_history.copy()
        return self.metrics_history[-last_n:] if self.metrics_history else []
    
    def get_average_metrics(self, last_n: int = 10) -> Optional[PerformanceMetrics]:
        """Pobierz Å›rednie metryki z ostatnich N pomiarÃ³w"""
        if not self.metrics_history:
            return None
        
        recent_metrics = self.get_metrics_history(last_n)
        if not recent_metrics:
            return None
        
        # Oblicz Å›rednie
        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory_used = sum(m.memory_used_mb for m in recent_metrics) / len(recent_metrics)
        avg_threads = sum(m.active_threads for m in recent_metrics) / len(recent_metrics)
        
        # GPU Å›rednie (jeÅ›li dostÄ™pne)
        gpu_metrics = [m for m in recent_metrics if m.gpu_memory_used_mb is not None]
        avg_gpu_used = (
            sum(m.gpu_memory_used_mb for m in gpu_metrics) / len(gpu_metrics)
            if gpu_metrics else None
        )
        avg_gpu_total = (
            recent_metrics[0].gpu_memory_total_mb 
            if recent_metrics and recent_metrics[0].gpu_memory_total_mb else None
        )
        
        return PerformanceMetrics(
            cpu_percent=avg_cpu,
            memory_percent=avg_memory,
            memory_used_mb=avg_memory_used,
            memory_available_mb=recent_metrics[-1].memory_available_mb,
            gpu_memory_used_mb=avg_gpu_used,
            gpu_memory_total_mb=avg_gpu_total,
            active_threads=int(avg_threads),
            queue_sizes=recent_metrics[-1].queue_sizes,
            processing_times=recent_metrics[-1].processing_times
        )
    
    def add_performance_callback(self, callback):
        """Dodaj callback dla metryki wydajnoÅ›ci"""
        self.performance_callbacks.append(callback)
        logger.info("ğŸ“Š Performance callback dodany")
    
    def remove_performance_callback(self, callback):
        """UsuÅ„ callback"""
        if callback in self.performance_callbacks:
            self.performance_callbacks.remove(callback)
            logger.info("ğŸ“Š Performance callback usuniÄ™ty")
    
    def get_optimization_recommendations(self) -> List[str]:
        """Pobierz rekomendacje optymalizacji"""
        recommendations = []
        
        current_metrics = self.get_current_metrics()
        if not current_metrics:
            return recommendations
        
        # Memory recommendations
        if current_metrics.memory_percent > 70:
            recommendations.append("ğŸ’¾ RozwaÅ¼ zamkniÄ™cie innych aplikacji aby zwolniÄ‡ pamiÄ™Ä‡")
            
        if current_metrics.memory_percent > 85:
            recommendations.append("ğŸš¨ Krytyczny poziom pamiÄ™ci - restart aplikacji zalecany")
        
        # CPU recommendations
        if current_metrics.cpu_percent > 70:
            recommendations.append("âš¡ Wysokie obciÄ…Å¼enie CPU - rozwaÅ¼ uÅ¼ycie mniejszego modelu STT")
            
        if current_metrics.cpu_percent > 85:
            recommendations.append("ğŸ”¥ PrzeciÄ…Å¼enie CPU - zamknij inne aplikacje")
        
        # GPU recommendations
        if (current_metrics.gpu_memory_used_mb and current_metrics.gpu_memory_total_mb):
            gpu_usage = current_metrics.gpu_memory_used_mb / current_metrics.gpu_memory_total_mb
            
            if gpu_usage > 0.8:
                recommendations.append("ğŸ”¥ Wysokie uÅ¼ycie pamiÄ™ci GPU - rozwaÅ¼ mniejszy model")
            
            if gpu_usage > 0.95:
                recommendations.append("ğŸš¨ Krytyczny poziom pamiÄ™ci GPU - zmniejsz batch size")
        
        # Thread recommendations
        if current_metrics.active_threads > 20:
            recommendations.append("ğŸ§µ DuÅ¼a liczba wÄ…tkÃ³w - moÅ¼liwa optymalizacja threading")
        
        return recommendations
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Wygeneruj raport wydajnoÅ›ci"""
        current = self.get_current_metrics()
        average = self.get_average_metrics()
        recommendations = self.get_optimization_recommendations()
        
        return {
            "timestamp": time.time(),
            "current_metrics": current.__dict__ if current else None,
            "average_metrics": average.__dict__ if average else None,
            "recommendations": recommendations,
            "monitoring_active": self.is_monitoring,
            "history_size": len(self.metrics_history),
            "optimization_settings": {
                "memory_optimization": self.optimize_memory,
                "cpu_optimization": True,  # Always enabled
                "gpu_optimization": True   # If available
            }
        }
    
    def __enter__(self):
        """Context manager entry"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.stop_monitoring()


class MemoryManager:
    """
    Manager pamiÄ™ci dla komponentÃ³w STT
    """
    
    def __init__(self):
        """Inicjalizacja memory managera"""
        self.cached_models = {}
        self.cache_limit_mb = 2000  # 2GB limit
        
    def cache_model(self, model_name: str, model_object):
        """Cachuj model w pamiÄ™ci"""
        try:
            # Oszacuj rozmiar modelu
            model_size_mb = self._estimate_model_size(model_object)
            
            # SprawdÅº czy mieÅ›ci siÄ™ w limicie
            if model_size_mb > self.cache_limit_mb:
                logger.warning(f"âš ï¸ Model {model_name} zbyt duÅ¼y dla cache: {model_size_mb}MB")
                return False
            
            # WyczyÅ›Ä‡ cache jeÅ›li potrzeba
            self._cleanup_cache_if_needed(model_size_mb)
            
            # Dodaj do cache
            self.cached_models[model_name] = {
                "model": model_object,
                "size_mb": model_size_mb,
                "last_used": time.time()
            }
            
            logger.info(f"ğŸ’¾ Model {model_name} cachowany ({model_size_mb}MB)")
            return True
            
        except Exception as e:
            logger.error(f"âŒ Model caching error: {e}")
            return False
    
    def get_cached_model(self, model_name: str):
        """Pobierz model z cache"""
        if model_name in self.cached_models:
            self.cached_models[model_name]["last_used"] = time.time()
            return self.cached_models[model_name]["model"]
        return None
    
    def _estimate_model_size(self, model_object) -> float:
        """Oszacuj rozmiar modelu w MB"""
        try:
            import torch
            if hasattr(model_object, "parameters"):
                total_params = sum(p.numel() for p in model_object.parameters())
                # ZaÅ‚Ã³Å¼ 4 bajty na parametr (float32)
                return (total_params * 4) / 1024**2
        except:
            pass
        
        # Fallback - domyÅ›lny rozmiar
        return 500.0  # MB
    
    def _cleanup_cache_if_needed(self, needed_mb: float):
        """WyczyÅ›Ä‡ cache jeÅ›li potrzeba miejsca"""
        current_size = sum(info["size_mb"] for info in self.cached_models.values())
        
        if current_size + needed_mb > self.cache_limit_mb:
            # Sortuj wedÅ‚ug ostatniego uÅ¼ycia
            sorted_models = sorted(
                self.cached_models.items(),
                key=lambda x: x[1]["last_used"]
            )
            
            # UsuÅ„ najstarsze modele
            for model_name, info in sorted_models:
                del self.cached_models[model_name]
                current_size -= info["size_mb"]
                logger.info(f"ğŸ—‘ï¸ UsuniÄ™to z cache: {model_name}")
                
                if current_size + needed_mb <= self.cache_limit_mb:
                    break


# Singleton instance
performance_optimizer = PerformanceOptimizer()
memory_manager = MemoryManager()
