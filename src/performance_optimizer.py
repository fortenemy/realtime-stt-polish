"""
Performance Optimizer for Real-time STT
Optymalizator wydajno≈õci dla Real-time STT

Autor: AI Assistant
Data: 2025-01-18
"""

import gc
import threading
import time
import psutil
import logging
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetrics:
    """Metryki wydajno≈õci systemu"""
    cpu_percent: float
    memory_percent: float
    memory_used_mb: float
    memory_available_mb: float
    gpu_memory_used_mb: Optional[float]
    gpu_memory_total_mb: Optional[float]
    active_threads: int
    queue_sizes: Dict[str, int]
    processing_times: Dict[str, float]
    
    @property
    def memory_usage_ratio(self) -> float:
        """Stosunek u≈ºywanej pamiƒôci do dostƒôpnej"""
        return self.memory_used_mb / max(self.memory_available_mb, 1)
    
    @property
    def is_memory_critical(self) -> bool:
        """Czy pamiƒôƒá jest na krytycznym poziomie"""
        return self.memory_percent > 85.0
    
    @property
    def is_cpu_overloaded(self) -> bool:
        """Czy CPU jest przeciƒÖ≈ºony"""
        return self.cpu_percent > 80.0

class PerformanceOptimizer:
    """
    Optymalizator wydajno≈õci dla Real-time STT
    """
    
    def __init__(
        self,
        enable_monitoring: bool = True,
        optimize_memory: bool = True,
        optimize_threads: bool = True,
        monitoring_interval: float = 1.0
    ):
        """
        Inicjalizacja optimizera
        
        Args:
            enable_monitoring: Czy w≈ÇƒÖczyƒá monitoring wydajno≈õci
            optimize_memory: Czy w≈ÇƒÖczyƒá optymalizacjƒô pamiƒôci
            optimize_threads: Czy w≈ÇƒÖczyƒá optymalizacjƒô wƒÖtk√≥w
            monitoring_interval: Interwa≈Ç monitoringu w sekundach
        """
        self.enable_monitoring = enable_monitoring
        self.optimize_memory = optimize_memory
        self.optimize_threads = optimize_threads
        self.monitoring_interval = monitoring_interval
        
        # Stan monitoringu
        self.is_monitoring = False
        self.monitoring_thread: Optional[threading.Thread] = None
        self.metrics_history: List[PerformanceMetrics] = []
        self.max_history_size = 60  # 1 minuta przy 1s interwale
        
        # Optymalizacje
        self.thread_pool: Optional[ThreadPoolExecutor] = None
        self.memory_threshold = 80.0  # %
        self.cpu_threshold = 75.0     # %
        
        # Callbacks
        self.performance_callbacks = []
        
        logger.info(f"üöÄ PerformanceOptimizer zainicjalizowany")
    
    def start_monitoring(self):
        """Rozpocznij monitoring wydajno≈õci"""
        if self.is_monitoring:
            logger.warning("‚ö†Ô∏è Monitoring ju≈º uruchomiony")
            return
        
        if not self.enable_monitoring:
            logger.info("üìä Monitoring wy≈ÇƒÖczony")
            return
        
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="PerformanceMonitor"
        )
        self.monitoring_thread.start()
        
        logger.info("üìä Monitoring wydajno≈õci uruchomiony")
    
    def stop_monitoring(self):
        """Zatrzymaj monitoring wydajno≈õci"""
        if not self.is_monitoring:
            return
        
        self.is_monitoring = False
        
        if self.monitoring_thread and self.monitoring_thread.is_alive():
            self.monitoring_thread.join(timeout=2.0)
        
        logger.info("üìä Monitoring wydajno≈õci zatrzymany")
    
    def _monitoring_loop(self):
        """G≈Ç√≥wna pƒôtla monitoringu"""
        while self.is_monitoring:
            try:
                # Zbierz metryki
                metrics = self.collect_metrics()
                
                # Dodaj do historii
                self.metrics_history.append(metrics)
                if len(self.metrics_history) > self.max_history_size:
                    self.metrics_history.pop(0)
                
                # Sprawd≈∫ czy potrzeba optymalizacji
                self._check_optimization_triggers(metrics)
                
                # Wywo≈Çaj callbacks
                for callback in self.performance_callbacks:
                    try:
                        callback(metrics)
                    except Exception as e:
                        logger.error(f"‚ùå Performance callback error: {e}")
                
                # Oczekaj do nastƒôpnego cyklu
                time.sleep(self.monitoring_interval)
                
            except Exception as e:
                logger.error(f"‚ùå Monitoring loop error: {e}")
                time.sleep(self.monitoring_interval)
    
    def collect_metrics(self) -> PerformanceMetrics:
        """Zbierz aktualne metryki wydajno≈õci"""
        try:
            # CPU i pamiƒôƒá systemu
            cpu_percent = psutil.cpu_percent(interval=None)
            memory = psutil.virtual_memory()
            
            # GPU metrics (je≈õli dostƒôpne)
            gpu_memory_used = None
            gpu_memory_total = None
            
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_memory_used = torch.cuda.memory_allocated() / 1024**2  # MB
                    gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2  # MB
            except ImportError:
                pass
            
            # WƒÖtki
            active_threads = threading.active_count()
            
            # Queue sizes (je≈õli dostƒôpne)
            queue_sizes = self._get_queue_sizes()
            
            # Processing times (placeholder)
            processing_times = self._get_processing_times()
            
            return PerformanceMetrics(
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_mb=memory.used / 1024**2,
                memory_available_mb=memory.available / 1024**2,
                gpu_memory_used_mb=gpu_memory_used,
                gpu_memory_total_mb=gpu_memory_total,
                active_threads=active_threads,
                queue_sizes=queue_sizes,
                processing_times=processing_times
            )
            
        except Exception as e:
            logger.error(f"‚ùå Metrics collection error: {e}")
            # Zwr√≥ƒá domy≈õlne metryki
            return PerformanceMetrics(
                cpu_percent=0.0,
                memory_percent=0.0,
                memory_used_mb=0.0,
                memory_available_mb=1000.0,
                gpu_memory_used_mb=None,
                gpu_memory_total_mb=None,
                active_threads=1,
                queue_sizes={},
                processing_times={}
            )
    
    def _get_queue_sizes(self) -> Dict[str, int]:
        """Pobierz rozmiary kolejek (implementacja specyficzna dla pipeline)"""
        # TODO: Integracja z RealtimePipeline
        return {}
    
    def _get_processing_times(self) -> Dict[str, float]:
        """Pobierz czasy przetwarzania"""
        # TODO: Integracja z komponentami
        return {}
    
    def _check_optimization_triggers(self, metrics: PerformanceMetrics):
        """Sprawd≈∫ czy potrzeba optymalizacji"""
        try:
            # Memory optimization
            if self.optimize_memory and metrics.is_memory_critical:
                logger.warning(f"‚ö†Ô∏è Krytyczny poziom pamiƒôci: {metrics.memory_percent:.1f}%")
                self.optimize_memory_usage()
            
            # CPU optimization
            if metrics.is_cpu_overloaded:
                logger.warning(f"‚ö†Ô∏è PrzeciƒÖ≈ºenie CPU: {metrics.cpu_percent:.1f}%")
                self.optimize_cpu_usage()
            
            # GPU memory optimization
            if (metrics.gpu_memory_used_mb and metrics.gpu_memory_total_mb and
                metrics.gpu_memory_used_mb / metrics.gpu_memory_total_mb > 0.9):
                logger.warning("‚ö†Ô∏è Krytyczny poziom pamiƒôci GPU")
                self.optimize_gpu_memory()
                
        except Exception as e:
            logger.error(f"‚ùå Optimization trigger error: {e}")
    
    def optimize_memory_usage(self):
        """Optymalizuj u≈ºycie pamiƒôci"""
        try:
            logger.info("üßπ Optymalizacja pamiƒôci...")
            
            # Wymu≈õ garbage collection
            collected = gc.collect()
            logger.info(f"üóëÔ∏è Garbage collection: {collected} obiekt√≥w")
            
            # GPU memory cleanup (je≈õli dostƒôpne)
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    logger.info("üî• GPU cache wyczyszczony")
            except ImportError:
                pass
            
            # TODO: Dodatkowe optymalizacje specyficzne dla aplikacji
            
        except Exception as e:
            logger.error(f"‚ùå Memory optimization error: {e}")
    
    def optimize_cpu_usage(self):
        """Optymalizuj u≈ºycie CPU"""
        try:
            logger.info("‚ö° Optymalizacja CPU...")
            
            # TODO: Implementacja optymalizacji CPU
            # - Redukcja czƒôstotliwo≈õci pr√≥bkowania
            # - Zwiƒôkszenie chunk size
            # - Ograniczenie liczby wƒÖtk√≥w
            
        except Exception as e:
            logger.error(f"‚ùå CPU optimization error: {e}")
    
    def optimize_gpu_memory(self):
        """Optymalizuj pamiƒôƒá GPU"""
        try:
            logger.info("üî• Optymalizacja pamiƒôci GPU...")
            
            import torch
            if torch.cuda.is_available():
                # Wyczy≈õƒá cache
                torch.cuda.empty_cache()
                
                # TODO: Dodatkowe optymalizacje GPU
                # - Redukcja batch size
                # - U≈ºycie half precision
                
        except ImportError:
            pass
        except Exception as e:
            logger.error(f"‚ùå GPU optimization error: {e}")
    
    def get_current_metrics(self) -> Optional[PerformanceMetrics]:
        """Pobierz aktualne metryki"""
        if self.metrics_history:
            return self.metrics_history[-1]
        return self.collect_metrics()
    
    def get_metrics_history(self, last_n: Optional[int] = None) -> List[PerformanceMetrics]:
        """Pobierz historiƒô metryki"""
        if last_n is None:
            return self.metrics_history.copy()
        return self.metrics_history[-last_n:] if self.metrics_history else []
    
    def get_average_metrics(self, last_n: int = 10) -> Optional[PerformanceMetrics]:
        """Pobierz ≈õrednie metryki z ostatnich N pomiar√≥w"""
        if not self.metrics_history:
            return None
        
        recent_metrics = self.get_metrics_history(last_n)
        if not recent_metrics:
            return None
        
        # Oblicz ≈õrednie
        avg_cpu = sum(m.cpu_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory = sum(m.memory_percent for m in recent_metrics) / len(recent_metrics)
        avg_memory_used = sum(m.memory_used_mb for m in recent_metrics) / len(recent_metrics)
        avg_threads = sum(m.active_threads for m in recent_metrics) / len(recent_metrics)
        
        # GPU ≈õrednie (je≈õli dostƒôpne)
        gpu_metrics = [m for m in recent_metrics if m.gpu_memory_used_mb is not None]
        avg_gpu_used = (
            sum(m.gpu_memory_used_mb for m in gpu_metrics) / len(gpu_metrics)
            if gpu_metrics else None
        )
        avg_gpu_total = (
            recent_metrics[0].gpu_memory_total_mb 
            if recent_metrics and recent_metrics[0].gpu_memory_total_mb else None
        )
        
        return PerformanceMetrics(
            cpu_percent=avg_cpu,
            memory_percent=avg_memory,
            memory_used_mb=avg_memory_used,
            memory_available_mb=recent_metrics[-1].memory_available_mb,
            gpu_memory_used_mb=avg_gpu_used,
            gpu_memory_total_mb=avg_gpu_total,
            active_threads=int(avg_threads),
            queue_sizes=recent_metrics[-1].queue_sizes,
            processing_times=recent_metrics[-1].processing_times
        )
    
    def add_performance_callback(self, callback):
        """Dodaj callback dla metryki wydajno≈õci"""
        self.performance_callbacks.append(callback)
        logger.info("üìä Performance callback dodany")
    
    def remove_performance_callback(self, callback):
        """Usu≈Ñ callback"""
        if callback in self.performance_callbacks:
            self.performance_callbacks.remove(callback)
            logger.info("üìä Performance callback usuniƒôty")
    
    def get_optimization_recommendations(self) -> List[str]:
        """Pobierz rekomendacje optymalizacji"""
        recommendations = []
        
        current_metrics = self.get_current_metrics()
        if not current_metrics:
            return recommendations
        
        # Memory recommendations
        if current_metrics.memory_percent > 70:
            recommendations.append("üíæ Rozwa≈º zamkniƒôcie innych aplikacji aby zwolniƒá pamiƒôƒá")
            
        if current_metrics.memory_percent > 85:
            recommendations.append("üö® Krytyczny poziom pamiƒôci - restart aplikacji zalecany")
        
        # CPU recommendations
        if current_metrics.cpu_percent > 70:
            recommendations.append("‚ö° Wysokie obciƒÖ≈ºenie CPU - rozwa≈º u≈ºycie mniejszego modelu STT")
            
        if current_metrics.cpu_percent > 85:
            recommendations.append("üî• PrzeciƒÖ≈ºenie CPU - zamknij inne aplikacje")
        
        # GPU recommendations
        if (current_metrics.gpu_memory_used_mb and current_metrics.gpu_memory_total_mb):
            gpu_usage = current_metrics.gpu_memory_used_mb / current_metrics.gpu_memory_total_mb
            
            if gpu_usage > 0.8:
                recommendations.append("üî• Wysokie u≈ºycie pamiƒôci GPU - rozwa≈º mniejszy model")
            
            if gpu_usage > 0.95:
                recommendations.append("üö® Krytyczny poziom pamiƒôci GPU - zmniejsz batch size")
        
        # Thread recommendations
        if current_metrics.active_threads > 20:
            recommendations.append("üßµ Du≈ºa liczba wƒÖtk√≥w - mo≈ºliwa optymalizacja threading")
        
        return recommendations
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """Wygeneruj raport wydajno≈õci"""
        current = self.get_current_metrics()
        average = self.get_average_metrics()
        recommendations = self.get_optimization_recommendations()
        
        return {
            "timestamp": time.time(),
            "current_metrics": current.__dict__ if current else None,
            "average_metrics": average.__dict__ if average else None,
            "recommendations": recommendations,
            "monitoring_active": self.is_monitoring,
            "history_size": len(self.metrics_history),
            "optimization_settings": {
                "memory_optimization": self.optimize_memory,
                "cpu_optimization": True,  # Always enabled
                "gpu_optimization": True   # If available
            }
        }
    
    def __enter__(self):
        """Context manager entry"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.stop_monitoring()


class MemoryManager:
    """
    Manager pamiƒôci dla komponent√≥w STT
    """
    
    def __init__(self):
        """Inicjalizacja memory managera"""
        self.cached_models = {}
        self.cache_limit_mb = 2000  # 2GB limit
        
    def cache_model(self, model_name: str, model_object):
        """Cachuj model w pamiƒôci"""
        try:
            # Oszacuj rozmiar modelu
            model_size_mb = self._estimate_model_size(model_object)
            
            # Sprawd≈∫ czy mie≈õci siƒô w limicie
            if model_size_mb > self.cache_limit_mb:
                logger.warning(f"‚ö†Ô∏è Model {model_name} zbyt du≈ºy dla cache: {model_size_mb}MB")
                return False
            
            # Wyczy≈õƒá cache je≈õli potrzeba
            self._cleanup_cache_if_needed(model_size_mb)
            
            # Dodaj do cache
            self.cached_models[model_name] = {
                "model": model_object,
                "size_mb": model_size_mb,
                "last_used": time.time()
            }
            
            logger.info(f"üíæ Model {model_name} cachowany ({model_size_mb}MB)")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Model caching error: {e}")
            return False
    
    def get_cached_model(self, model_name: str):
        """Pobierz model z cache"""
        if model_name in self.cached_models:
            self.cached_models[model_name]["last_used"] = time.time()
            return self.cached_models[model_name]["model"]
        return None
    
    def _estimate_model_size(self, model_object) -> float:
        """Oszacuj rozmiar modelu w MB"""
        try:
            import torch
            if hasattr(model_object, "parameters"):
                total_params = sum(p.numel() for p in model_object.parameters())
                # Za≈Ç√≥≈º 4 bajty na parametr (float32)
                return (total_params * 4) / 1024**2
        except:
            pass
        
        # Fallback - domy≈õlny rozmiar
        return 500.0  # MB
    
    def _cleanup_cache_if_needed(self, needed_mb: float):
        """Wyczy≈õƒá cache je≈õli potrzeba miejsca"""
        current_size = sum(info["size_mb"] for info in self.cached_models.values())
        
        if current_size + needed_mb > self.cache_limit_mb:
            # Sortuj wed≈Çug ostatniego u≈ºycia
            sorted_models = sorted(
                self.cached_models.items(),
                key=lambda x: x[1]["last_used"]
            )
            
            # Usu≈Ñ najstarsze modele
            for model_name, info in sorted_models:
                del self.cached_models[model_name]
                current_size -= info["size_mb"]
                logger.info(f"üóëÔ∏è Usuniƒôto z cache: {model_name}")
                
                if current_size + needed_mb <= self.cache_limit_mb:
                    break


# Singleton instance
performance_optimizer = PerformanceOptimizer()
memory_manager = MemoryManager()
