#!/usr/bin/env python3
"""
Kompletny test systemu Real-time STT Polish
Complete system test for Real-time STT Polish

Autor: AI Assistant
Data: 2025-01-18
"""

import sys
import time
import tempfile
import json
from pathlib import Path
from typing import List, Dict, Any

# Dodaj src do Å›cieÅ¼ki
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_all_components():
    """Test wszystkich komponentÃ³w systemu"""
    print("ğŸ§ª Test wszystkich komponentÃ³w")
    print("=" * 50)
    
    components_results = {}
    
    # Test AudioCapture
    try:
        from audio_capture import AudioCapture
        capture = AudioCapture()
        components_results["AudioCapture"] = True
        print("âœ… AudioCapture - OK")
    except Exception as e:
        components_results["AudioCapture"] = False
        print(f"âŒ AudioCapture - {e}")
    
    # Test VAD
    try:
        from voice_activity_detector import SimpleVAD, WebRTCVAD, VADMode
        vad = SimpleVAD()
        components_results["VAD"] = True
        print("âœ… Voice Activity Detection - OK")
    except Exception as e:
        components_results["VAD"] = False
        print(f"âŒ VAD - {e}")
    
    # Test STT Engine
    try:
        from stt_engine import WhisperSTTEngine, PolishOptimizedSTT
        # Nie Å‚aduj modelu, tylko test klasy
        engine = WhisperSTTEngine(model_name="tiny")
        components_results["STT_Engine"] = True
        print("âœ… STT Engine - OK")
    except Exception as e:
        components_results["STT_Engine"] = False
        print(f"âŒ STT Engine - {e}")
    
    # Test Pipeline
    try:
        from realtime_pipeline import RealtimeSTTPipeline, SpeechSegment
        pipeline = RealtimeSTTPipeline(enable_stt=False)  # Bez STT dla testu
        components_results["Pipeline"] = True
        print("âœ… RealtimePipeline - OK")
    except Exception as e:
        components_results["Pipeline"] = False
        print(f"âŒ Pipeline - {e}")
    
    # Test Performance Optimizer
    try:
        from performance_optimizer import PerformanceOptimizer, MemoryManager
        optimizer = PerformanceOptimizer(enable_monitoring=False)
        components_results["Performance"] = True
        print("âœ… Performance Optimizer - OK")
    except Exception as e:
        components_results["Performance"] = False
        print(f"âŒ Performance Optimizer - {e}")
    
    # Test Export Manager
    try:
        from export_manager import ExportManager
        exporter = ExportManager()
        components_results["Export"] = True
        print("âœ… Export Manager - OK")
    except Exception as e:
        components_results["Export"] = False
        print(f"âŒ Export Manager - {e}")
    
    # Test GUI Application
    try:
        from gui_application import STTGuiApplication
        # Nie tworzymy instancji GUI w teÅ›cie
        components_results["GUI"] = True
        print("âœ… GUI Application - OK")
    except Exception as e:
        components_results["GUI"] = False
        print(f"âŒ GUI Application - {e}")
    
    return components_results

def test_export_functionality():
    """Test funkcjonalnoÅ›ci eksportu"""
    print("\nğŸ“¤ Test funkcjonalnoÅ›ci eksportu")
    print("=" * 40)
    
    try:
        from export_manager import ExportManager
        from realtime_pipeline import SpeechSegment
        from stt_engine import TranscriptionResult
        
        # StwÃ³rz testowe segmenty
        test_segments = []
        
        for i in range(3):
            # StwÃ³rz fake transcription
            transcription = TranscriptionResult(
                text=f"To jest testowy segment numer {i+1}.",
                language="pl",
                confidence=0.95 - i*0.1,
                processing_time=0.1 + i*0.05,
                segments=[],
                model_used="test"
            )
            
            # StwÃ³rz segment
            segment = SpeechSegment(
                audio_data=None,  # Nie potrzebujemy audio dla testu eksportu
                start_time=float(i * 3),
                end_time=float(i * 3 + 2),
                confidence=0.95,
                sample_rate=16000,
                transcription=transcription
            )
            
            test_segments.append(segment)
        
        # Test eksportu
        exporter = ExportManager()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Test rÃ³Å¼nych formatÃ³w
            formats_to_test = ["txt", "json", "csv", "srt", "vtt", "xml"]
            results = {}
            
            for fmt in formats_to_test:
                output_file = temp_path / f"test.{fmt}"
                success = exporter.export_transcription(
                    test_segments, str(output_file), fmt
                )
                results[fmt] = success
                
                if success:
                    # SprawdÅº czy plik zostaÅ‚ utworzony
                    if output_file.exists() and output_file.stat().st_size > 0:
                        print(f"âœ… Export {fmt} - OK ({output_file.stat().st_size} bytes)")
                    else:
                        print(f"âŒ Export {fmt} - plik pusty")
                        results[fmt] = False
                else:
                    print(f"âŒ Export {fmt} - bÅ‚Ä…d")
            
            # Test batch export
            batch_results = exporter.batch_export(
                test_segments, str(temp_path), ["txt", "json", "csv"]
            )
            
            print(f"ğŸ“¦ Batch export: {sum(batch_results.values())}/{len(batch_results)} formatÃ³w")
            
            return all(results.values())
        
    except Exception as e:
        print(f"âŒ Export test failed: {e}")
        return False

def test_performance_monitoring():
    """Test monitoringu wydajnoÅ›ci"""
    print("\nğŸ“Š Test monitoringu wydajnoÅ›ci")
    print("=" * 40)
    
    try:
        from performance_optimizer import PerformanceOptimizer
        
        optimizer = PerformanceOptimizer(
            enable_monitoring=True,
            monitoring_interval=0.1  # Szybki test
        )
        
        # Uruchom monitoring na krÃ³tko
        optimizer.start_monitoring()
        time.sleep(0.5)  # PÃ³Å‚ sekundy monitoringu
        
        # SprawdÅº metryki
        current_metrics = optimizer.get_current_metrics()
        if current_metrics:
            print(f"âœ… CPU: {current_metrics.cpu_percent:.1f}%")
            print(f"âœ… Memory: {current_metrics.memory_percent:.1f}%")
            print(f"âœ… Threads: {current_metrics.active_threads}")
            
            # Test rekomendacji
            recommendations = optimizer.get_optimization_recommendations()
            print(f"âœ… Recommendations: {len(recommendations)} items")
            
            # Test raportu
            report = optimizer.generate_performance_report()
            print(f"âœ… Performance report generated")
            
        optimizer.stop_monitoring()
        
        return current_metrics is not None
        
    except Exception as e:
        print(f"âŒ Performance monitoring test failed: {e}")
        return False

def test_configuration_system():
    """Test systemu konfiguracji"""
    print("\nâš™ï¸ Test systemu konfiguracji")
    print("=" * 40)
    
    try:
        # Test konfiguracji main aplikacji
        test_config = {
            "model": "base",
            "language": "pl",
            "vad_mode": "normal",
            "min_segment_duration": 1.0,
            "silence_timeout": 2.0,
            "enable_stt": True,
            "use_polish_optimization": True
        }
        
        # Test zapisu i odczytu
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(test_config, f)
            config_path = f.name
        
        # Odczytaj konfiguracjÄ™
        with open(config_path, 'r') as f:
            loaded_config = json.load(f)
        
        # SprawdÅº czy konfiguracja jest identyczna
        if loaded_config == test_config:
            print("âœ… Configuration save/load - OK")
            
            # Test walidacji konfiguracji
            required_keys = ["model", "language", "vad_mode"]
            valid_config = all(key in loaded_config for key in required_keys)
            
            if valid_config:
                print("âœ… Configuration validation - OK")
                return True
            else:
                print("âŒ Configuration validation - missing keys")
                return False
        else:
            print("âŒ Configuration save/load - mismatch")
            return False
        
    except Exception as e:
        print(f"âŒ Configuration test failed: {e}")
        return False
    finally:
        # Cleanup
        try:
            Path(config_path).unlink()
        except:
            pass

def test_integration_readiness():
    """Test gotowoÅ›ci do peÅ‚nej integracji"""
    print("\nğŸ”— Test gotowoÅ›ci integracji")
    print("=" * 40)
    
    integration_checks = []
    
    # Check 1: Wszystkie moduÅ‚y importowalne
    try:
        modules = [
            "audio_capture", "voice_activity_detector", "stt_engine",
            "realtime_pipeline", "performance_optimizer", "export_manager",
            "gui_application"
        ]
        
        for module in modules:
            __import__(module)
        
        print("âœ… All modules importable")
        integration_checks.append(True)
    except Exception as e:
        print(f"âŒ Module import failed: {e}")
        integration_checks.append(False)
    
    # Check 2: Pipeline moÅ¼e byÄ‡ utworzony
    try:
        from realtime_pipeline import RealtimeSTTPipeline
        pipeline = RealtimeSTTPipeline(enable_stt=False)
        print("âœ… Pipeline creation")
        integration_checks.append(True)
    except Exception as e:
        print(f"âŒ Pipeline creation failed: {e}")
        integration_checks.append(False)
    
    # Check 3: Export manager funkcjonalny
    try:
        from export_manager import ExportManager
        exporter = ExportManager()
        formats = exporter.get_supported_formats()
        if len(formats) >= 5:
            print(f"âœ… Export manager ({len(formats)} formats)")
            integration_checks.append(True)
        else:
            print(f"âŒ Export manager insufficient formats")
            integration_checks.append(False)
    except Exception as e:
        print(f"âŒ Export manager failed: {e}")
        integration_checks.append(False)
    
    # Check 4: Performance monitoring
    try:
        from performance_optimizer import PerformanceOptimizer
        optimizer = PerformanceOptimizer(enable_monitoring=False)
        metrics = optimizer.collect_metrics()
        if metrics.cpu_percent >= 0:
            print("âœ… Performance monitoring")
            integration_checks.append(True)
        else:
            print("âŒ Performance monitoring invalid")
            integration_checks.append(False)
    except Exception as e:
        print(f"âŒ Performance monitoring failed: {e}")
        integration_checks.append(False)
    
    success_rate = sum(integration_checks) / len(integration_checks)
    print(f"\nğŸ“Š Integration readiness: {success_rate:.1%}")
    
    return success_rate >= 0.8

def test_system_requirements():
    """Test wymagaÅ„ systemowych"""
    print("\nğŸ’» Test wymagaÅ„ systemowych")
    print("=" * 40)
    
    requirements_met = []
    
    # Python version
    if sys.version_info >= (3, 8):
        print(f"âœ… Python {sys.version_info.major}.{sys.version_info.minor}")
        requirements_met.append(True)
    else:
        print(f"âŒ Python {sys.version_info.major}.{sys.version_info.minor} (wymagany 3.8+)")
        requirements_met.append(False)
    
    # Required packages
    required_packages = [
        ("numpy", "1.20.0"),
        ("sounddevice", "0.4.0"),
        ("psutil", "5.0.0")
    ]
    
    for package, min_version in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "unknown")
            print(f"âœ… {package} {version}")
            requirements_met.append(True)
        except ImportError:
            print(f"âŒ {package} missing")
            requirements_met.append(False)
    
    # Optional packages (for full functionality)
    optional_packages = [
        ("whisper", "OpenAI Whisper"),
        ("torch", "PyTorch"),
        ("tkinter", "GUI support")
    ]
    
    print("\nğŸ“¦ Optional packages:")
    for package, description in optional_packages:
        try:
            __import__(package)
            print(f"âœ… {package} ({description})")
        except ImportError:
            print(f"âš ï¸ {package} missing ({description})")
    
    # System resources
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / 1024**3
        cpu_count = psutil.cpu_count()
        
        print(f"\nğŸ’¾ RAM: {memory_gb:.1f}GB")
        print(f"ğŸ–¥ï¸ CPU cores: {cpu_count}")
        
        if memory_gb >= 4:
            print("âœ… Sufficient RAM")
            requirements_met.append(True)
        else:
            print("âš ï¸ Low RAM (4GB+ recommended)")
            requirements_met.append(False)
        
        if cpu_count >= 2:
            print("âœ… Sufficient CPU cores")
            requirements_met.append(True)
        else:
            print("âš ï¸ Low CPU cores (2+ recommended)")
            requirements_met.append(False)
            
    except Exception as e:
        print(f"âŒ System resource check failed: {e}")
        requirements_met.append(False)
    
    return all(requirements_met)

def generate_system_report():
    """Wygeneruj raport systemu"""
    print("\nğŸ“‹ Generowanie raportu systemu")
    print("=" * 40)
    
    report = {
        "timestamp": time.time(),
        "test_results": {},
        "system_info": {},
        "recommendations": []
    }
    
    # Uruchom wszystkie testy
    tests = [
        ("Components", test_all_components),
        ("Export", test_export_functionality),
        ("Performance", test_performance_monitoring),
        ("Configuration", test_configuration_system),
        ("Integration", test_integration_readiness),
        ("Requirements", test_system_requirements)
    ]
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            report["test_results"][test_name] = result
        except Exception as e:
            print(f"âŒ {test_name} test crashed: {e}")
            report["test_results"][test_name] = False
    
    # System info
    try:
        import platform
        import psutil
        
        report["system_info"] = {
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            "platform": platform.platform(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / 1024**3,
            "architecture": platform.architecture()[0]
        }
    except Exception as e:
        report["system_info"]["error"] = str(e)
    
    # Recommendations
    passed_tests = sum(report["test_results"].values())
    total_tests = len(report["test_results"])
    
    if passed_tests == total_tests:
        report["recommendations"].append("ğŸ‰ System jest w peÅ‚ni gotowy do uÅ¼ycia!")
        report["recommendations"].append("MoÅ¼esz uruchomiÄ‡: python main.py --mode demo")
    else:
        report["recommendations"].append("âš ï¸ NiektÃ³re komponenty wymagajÄ… naprawy")
        
        if not report["test_results"].get("Requirements", True):
            report["recommendations"].append("Zainstaluj brakujÄ…ce dependencies")
        
        if not report["test_results"].get("Components", True):
            report["recommendations"].append("SprawdÅº importy moduÅ‚Ã³w")
        
        if not report["test_results"].get("Integration", True):
            report["recommendations"].append("Napraw bÅ‚Ä™dy integracji")
    
    # Zapisz raport
    try:
        report_path = "system_test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Raport zapisany: {report_path}")
    except Exception as e:
        print(f"âŒ Nie moÅ¼na zapisaÄ‡ raportu: {e}")
    
    return report

def main():
    """GÅ‚Ã³wna funkcja testÃ³w"""
    print("ğŸ§ª Real-time STT Polish - Kompletny Test Systemu")
    print("=" * 60)
    print("ğŸ¤ Testowanie wszystkich komponentÃ³w systemu...")
    print()
    
    # Wygeneruj raport
    report = generate_system_report()
    
    # Podsumowanie
    print("\n" + "=" * 60)
    print("ğŸ“Š PODSUMOWANIE TESTÃ“W SYSTEMU")
    print("=" * 60)
    
    passed = sum(report["test_results"].values())
    total = len(report["test_results"])
    
    for test_name, result in report["test_results"].items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nğŸ¯ Wyniki: {passed}/{total} testÃ³w przeszÅ‚o ({passed/total:.1%})")
    
    # Rekomendacje
    print("\nğŸ’¡ REKOMENDACJE:")
    for rec in report["recommendations"]:
        print(f"   {rec}")
    
    if passed == total:
        print("\nğŸ‰ SYSTEM W PEÅNI FUNKCJONALNY!")
        print("ğŸš€ Real-time Speech-to-Text Polish gotowy do uÅ¼ycia!")
        print("\nğŸ“‹ NastÄ™pne kroki:")
        print("   1. python main.py --mode demo")
        print("   2. python gui_launcher.py")
        print("   3. python install_whisper_dependencies.py (jeÅ›li Whisper nie jest zainstalowany)")
        return True
    else:
        print("\nâš ï¸ System wymaga poprawek przed uÅ¼yciem")
        print("ğŸ“‹ SprawdÅº bÅ‚Ä™dy powyÅ¼ej i napraw problemy")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ Test przerwany przez uÅ¼ytkownika")
        sys.exit(0)
