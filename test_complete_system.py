#!/usr/bin/env python3
"""
Kompletny test systemu Real-time STT Polish
Complete system test for Real-time STT Polish

Autor: AI Assistant
Data: 2025-01-18
"""

import sys
import time
import tempfile
import json
from pathlib import Path
from typing import List, Dict, Any

# Dodaj src do ≈õcie≈ºki
sys.path.insert(0, str(Path(__file__).parent / "src"))

def test_all_components():
    """Test wszystkich komponent√≥w systemu"""
    print("üß™ Test wszystkich komponent√≥w")
    print("=" * 50)
    
    components_results = {}
    
    # Test AudioCapture
    try:
        from audio_capture import AudioCapture
        capture = AudioCapture()
        components_results["AudioCapture"] = True
        print("‚úÖ AudioCapture - OK")
    except Exception as e:
        components_results["AudioCapture"] = False
        print(f"‚ùå AudioCapture - {e}")
    
    # Test VAD
    try:
        from voice_activity_detector import SimpleVAD, WebRTCVAD, VADMode
        vad = SimpleVAD()
        components_results["VAD"] = True
        print("‚úÖ Voice Activity Detection - OK")
    except Exception as e:
        components_results["VAD"] = False
        print(f"‚ùå VAD - {e}")
    
    # Test STT Engine
    try:
        from stt_engine import WhisperSTTEngine, PolishOptimizedSTT
        # Nie ≈Çaduj modelu, tylko test klasy
        engine = WhisperSTTEngine(model_name="tiny")
        components_results["STT_Engine"] = True
        print("‚úÖ STT Engine - OK")
    except Exception as e:
        components_results["STT_Engine"] = False
        print(f"‚ùå STT Engine - {e}")
    
    # Test Pipeline
    try:
        from realtime_pipeline import RealtimeSTTPipeline, SpeechSegment
        pipeline = RealtimeSTTPipeline(enable_stt=False)  # Bez STT dla testu
        components_results["Pipeline"] = True
        print("‚úÖ RealtimePipeline - OK")
    except Exception as e:
        components_results["Pipeline"] = False
        print(f"‚ùå Pipeline - {e}")
    
    # Test Performance Optimizer
    try:
        from performance_optimizer import PerformanceOptimizer, MemoryManager
        optimizer = PerformanceOptimizer(enable_monitoring=False)
        components_results["Performance"] = True
        print("‚úÖ Performance Optimizer - OK")
    except Exception as e:
        components_results["Performance"] = False
        print(f"‚ùå Performance Optimizer - {e}")
    
    # Test Export Manager
    try:
        from export_manager import ExportManager
        exporter = ExportManager()
        components_results["Export"] = True
        print("‚úÖ Export Manager - OK")
    except Exception as e:
        components_results["Export"] = False
        print(f"‚ùå Export Manager - {e}")
    
    # Test GUI Application
    try:
        from gui_application import STTGuiApplication
        # Nie tworzymy instancji GUI w te≈õcie
        components_results["GUI"] = True
        print("‚úÖ GUI Application - OK")
    except Exception as e:
        components_results["GUI"] = False
        print(f"‚ùå GUI Application - {e}")
    
    return components_results

def test_export_functionality():
    """Test funkcjonalno≈õci eksportu"""
    print("\nüì§ Test funkcjonalno≈õci eksportu")
    print("=" * 40)
    
    try:
        from export_manager import ExportManager
        from realtime_pipeline import SpeechSegment
        from stt_engine import TranscriptionResult
        
        # Stw√≥rz testowe segmenty
        test_segments = []
        
        for i in range(3):
            # Stw√≥rz fake transcription
            transcription = TranscriptionResult(
                text=f"To jest testowy segment numer {i+1}.",
                language="pl",
                confidence=0.95 - i*0.1,
                processing_time=0.1 + i*0.05,
                segments=[],
                model_used="test"
            )
            
            # Stw√≥rz segment
            segment = SpeechSegment(
                audio_data=None,  # Nie potrzebujemy audio dla testu eksportu
                start_time=float(i * 3),
                end_time=float(i * 3 + 2),
                confidence=0.95,
                sample_rate=16000,
                transcription=transcription
            )
            
            test_segments.append(segment)
        
        # Test eksportu
        exporter = ExportManager()
        
        with tempfile.TemporaryDirectory() as temp_dir:
            temp_path = Path(temp_dir)
            
            # Test r√≥≈ºnych format√≥w
            formats_to_test = ["txt", "json", "csv", "srt", "vtt", "xml"]
            results = {}
            
            for fmt in formats_to_test:
                output_file = temp_path / f"test.{fmt}"
                success = exporter.export_transcription(
                    test_segments, str(output_file), fmt
                )
                results[fmt] = success
                
                if success:
                    # Sprawd≈∫ czy plik zosta≈Ç utworzony
                    if output_file.exists() and output_file.stat().st_size > 0:
                        print(f"‚úÖ Export {fmt} - OK ({output_file.stat().st_size} bytes)")
                    else:
                        print(f"‚ùå Export {fmt} - plik pusty")
                        results[fmt] = False
                else:
                    print(f"‚ùå Export {fmt} - b≈ÇƒÖd")
            
            # Test batch export
            batch_results = exporter.batch_export(
                test_segments, str(temp_path), ["txt", "json", "csv"]
            )
            
            print(f"üì¶ Batch export: {sum(batch_results.values())}/{len(batch_results)} format√≥w")
            
            return all(results.values())
        
    except Exception as e:
        print(f"‚ùå Export test failed: {e}")
        return False

def test_performance_monitoring():
    """Test monitoringu wydajno≈õci"""
    print("\nüìä Test monitoringu wydajno≈õci")
    print("=" * 40)
    
    try:
        from performance_optimizer import PerformanceOptimizer
        
        optimizer = PerformanceOptimizer(
            enable_monitoring=True,
            monitoring_interval=0.1  # Szybki test
        )
        
        # Uruchom monitoring na kr√≥tko
        optimizer.start_monitoring()
        time.sleep(0.5)  # P√≥≈Ç sekundy monitoringu
        
        # Sprawd≈∫ metryki
        current_metrics = optimizer.get_current_metrics()
        if current_metrics:
            print(f"‚úÖ CPU: {current_metrics.cpu_percent:.1f}%")
            print(f"‚úÖ Memory: {current_metrics.memory_percent:.1f}%")
            print(f"‚úÖ Threads: {current_metrics.active_threads}")
            
            # Test rekomendacji
            recommendations = optimizer.get_optimization_recommendations()
            print(f"‚úÖ Recommendations: {len(recommendations)} items")
            
            # Test raportu
            report = optimizer.generate_performance_report()
            print(f"‚úÖ Performance report generated")
            
        optimizer.stop_monitoring()
        
        return current_metrics is not None
        
    except Exception as e:
        print(f"‚ùå Performance monitoring test failed: {e}")
        return False

def test_configuration_system():
    """Test systemu konfiguracji"""
    print("\n‚öôÔ∏è Test systemu konfiguracji")
    print("=" * 40)
    
    try:
        # Test konfiguracji main aplikacji
        test_config = {
            "model": "base",
            "language": "pl",
            "vad_mode": "normal",
            "min_segment_duration": 1.0,
            "silence_timeout": 2.0,
            "enable_stt": True,
            "use_polish_optimization": True
        }
        
        # Test zapisu i odczytu
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(test_config, f)
            config_path = f.name
        
        # Odczytaj konfiguracjƒô
        with open(config_path, 'r') as f:
            loaded_config = json.load(f)
        
        # Sprawd≈∫ czy konfiguracja jest identyczna
        if loaded_config == test_config:
            print("‚úÖ Configuration save/load - OK")
            
            # Test walidacji konfiguracji
            required_keys = ["model", "language", "vad_mode"]
            valid_config = all(key in loaded_config for key in required_keys)
            
            if valid_config:
                print("‚úÖ Configuration validation - OK")
                return True
            else:
                print("‚ùå Configuration validation - missing keys")
                return False
        else:
            print("‚ùå Configuration save/load - mismatch")
            return False
        
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False
    finally:
        # Cleanup
        try:
            Path(config_path).unlink()
        except:
            pass

def test_integration_readiness():
    """Test gotowo≈õci do pe≈Çnej integracji"""
    print("\nüîó Test gotowo≈õci integracji")
    print("=" * 40)
    
    integration_checks = []
    
    # Check 1: Wszystkie modu≈Çy importowalne
    try:
        modules = [
            "audio_capture", "voice_activity_detector", "stt_engine",
            "realtime_pipeline", "performance_optimizer", "export_manager",
            "gui_application"
        ]
        
        for module in modules:
            __import__(module)
        
        print("‚úÖ All modules importable")
        integration_checks.append(True)
    except Exception as e:
        print(f"‚ùå Module import failed: {e}")
        integration_checks.append(False)
    
    # Check 2: Pipeline mo≈ºe byƒá utworzony
    try:
        from realtime_pipeline import RealtimeSTTPipeline
        pipeline = RealtimeSTTPipeline(enable_stt=False)
        print("‚úÖ Pipeline creation")
        integration_checks.append(True)
    except Exception as e:
        print(f"‚ùå Pipeline creation failed: {e}")
        integration_checks.append(False)
    
    # Check 3: Export manager funkcjonalny
    try:
        from export_manager import ExportManager
        exporter = ExportManager()
        formats = exporter.get_supported_formats()
        if len(formats) >= 5:
            print(f"‚úÖ Export manager ({len(formats)} formats)")
            integration_checks.append(True)
        else:
            print(f"‚ùå Export manager insufficient formats")
            integration_checks.append(False)
    except Exception as e:
        print(f"‚ùå Export manager failed: {e}")
        integration_checks.append(False)
    
    # Check 4: Performance monitoring
    try:
        from performance_optimizer import PerformanceOptimizer
        optimizer = PerformanceOptimizer(enable_monitoring=False)
        metrics = optimizer.collect_metrics()
        if metrics.cpu_percent >= 0:
            print("‚úÖ Performance monitoring")
            integration_checks.append(True)
        else:
            print("‚ùå Performance monitoring invalid")
            integration_checks.append(False)
    except Exception as e:
        print(f"‚ùå Performance monitoring failed: {e}")
        integration_checks.append(False)
    
    success_rate = sum(integration_checks) / len(integration_checks)
    print(f"\nüìä Integration readiness: {success_rate:.1%}")
    
    return success_rate >= 0.8

def test_system_requirements():
    """Test wymaga≈Ñ systemowych"""
    print("\nüíª Test wymaga≈Ñ systemowych")
    print("=" * 40)
    
    requirements_met = []
    
    # Python version
    if sys.version_info >= (3, 8):
        print(f"‚úÖ Python {sys.version_info.major}.{sys.version_info.minor}")
        requirements_met.append(True)
    else:
        print(f"‚ùå Python {sys.version_info.major}.{sys.version_info.minor} (wymagany 3.8+)")
        requirements_met.append(False)
    
    # Required packages
    required_packages = [
        ("numpy", "1.20.0"),
        ("sounddevice", "0.4.0"),
        ("psutil", "5.0.0")
    ]
    
    for package, min_version in required_packages:
        try:
            module = __import__(package)
            version = getattr(module, "__version__", "unknown")
            print(f"‚úÖ {package} {version}")
            requirements_met.append(True)
        except ImportError:
            print(f"‚ùå {package} missing")
            requirements_met.append(False)
    
    # Optional packages (for full functionality)
    optional_packages = [
        ("whisper", "OpenAI Whisper"),
        ("torch", "PyTorch"),
        ("tkinter", "GUI support")
    ]
    
    print("\nüì¶ Optional packages:")
    for package, description in optional_packages:
        try:
            __import__(package)
            print(f"‚úÖ {package} ({description})")
        except ImportError:
            print(f"‚ö†Ô∏è {package} missing ({description})")
    
    # System resources
    try:
        import psutil
        memory_gb = psutil.virtual_memory().total / 1024**3
        cpu_count = psutil.cpu_count()
        
        print(f"\nüíæ RAM: {memory_gb:.1f}GB")
        print(f"üñ•Ô∏è CPU cores: {cpu_count}")
        
        if memory_gb >= 4:
            print("‚úÖ Sufficient RAM")
            requirements_met.append(True)
        else:
            print("‚ö†Ô∏è Low RAM (4GB+ recommended)")
            requirements_met.append(False)
        
        if cpu_count >= 2:
            print("‚úÖ Sufficient CPU cores")
            requirements_met.append(True)
        else:
            print("‚ö†Ô∏è Low CPU cores (2+ recommended)")
            requirements_met.append(False)
            
    except Exception as e:
        print(f"‚ùå System resource check failed: {e}")
        requirements_met.append(False)
    
    return all(requirements_met)

def generate_system_report():
    """Wygeneruj raport systemu"""
    print("\nüìã Generowanie raportu systemu")
    print("=" * 40)
    
    report = {
        "timestamp": time.time(),
        "test_results": {},
        "system_info": {},
        "recommendations": []
    }
    
    # Uruchom wszystkie testy
    tests = [
        ("Components", test_all_components),
        ("Export", test_export_functionality),
        ("Performance", test_performance_monitoring),
        ("Configuration", test_configuration_system),
        ("Integration", test_integration_readiness),
        ("Requirements", test_system_requirements)
    ]
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            report["test_results"][test_name] = result
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            report["test_results"][test_name] = False
    
    # System info
    try:
        import platform
        import psutil
        
        report["system_info"] = {
            "python_version": f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
            "platform": platform.platform(),
            "cpu_count": psutil.cpu_count(),
            "memory_gb": psutil.virtual_memory().total / 1024**3,
            "architecture": platform.architecture()[0]
        }
    except Exception as e:
        report["system_info"]["error"] = str(e)
    
    # Recommendations
    passed_tests = sum(report["test_results"].values())
    total_tests = len(report["test_results"])
    
    if passed_tests == total_tests:
        report["recommendations"].append("üéâ System jest w pe≈Çni gotowy do u≈ºycia!")
        report["recommendations"].append("Mo≈ºesz uruchomiƒá: python main.py --mode demo")
    else:
        report["recommendations"].append("‚ö†Ô∏è Niekt√≥re komponenty wymagajƒÖ naprawy")
        
        if not report["test_results"].get("Requirements", True):
            report["recommendations"].append("Zainstaluj brakujƒÖce dependencies")
        
        if not report["test_results"].get("Components", True):
            report["recommendations"].append("Sprawd≈∫ importy modu≈Ç√≥w")
        
        if not report["test_results"].get("Integration", True):
            report["recommendations"].append("Napraw b≈Çƒôdy integracji")
    
    # Zapisz raport
    try:
        report_path = "system_test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Raport zapisany: {report_path}")
    except Exception as e:
        print(f"‚ùå Nie mo≈ºna zapisaƒá raportu: {e}")
    
    return report

def main():
    """G≈Ç√≥wna funkcja test√≥w"""
    print("üß™ Real-time STT Polish - Kompletny Test Systemu")
    print("=" * 60)
    print("üé§ Testowanie wszystkich komponent√≥w systemu...")
    print()
    
    # Wygeneruj raport
    report = generate_system_report()
    
    # Podsumowanie
    print("\n" + "=" * 60)
    print("üìä PODSUMOWANIE TEST√ìW SYSTEMU")
    print("=" * 60)
    
    passed = sum(report["test_results"].values())
    total = len(report["test_results"])
    
    for test_name, result in report["test_results"].items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status} {test_name}")
    
    print(f"\nüéØ Wyniki: {passed}/{total} test√≥w przesz≈Ço ({passed/total:.1%})")
    
    # Rekomendacje
    print("\nüí° REKOMENDACJE:")
    for rec in report["recommendations"]:
        print(f"   {rec}")
    
    if passed == total:
        print("\nüéâ SYSTEM W PE≈ÅNI FUNKCJONALNY!")
        print("üöÄ Real-time Speech-to-Text Polish gotowy do u≈ºycia!")
        print("\nüìã Nastƒôpne kroki:")
        print("   1. python main.py --mode demo")
        print("   2. python gui_launcher.py")
        print("   3. python install_whisper_dependencies.py (je≈õli Whisper nie jest zainstalowany)")
        return True
    else:
        print("\n‚ö†Ô∏è System wymaga poprawek przed u≈ºyciem")
        print("üìã Sprawd≈∫ b≈Çƒôdy powy≈ºej i napraw problemy")
        return False

if __name__ == "__main__":
    try:
        success = main()
        sys.exit(0 if success else 1)
    except KeyboardInterrupt:
        print("\nüëã Test przerwany przez u≈ºytkownika")
        sys.exit(0)
